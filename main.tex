\input{preamble.tex}
\addbibresource{references.bib}
\hypersetup{
    pdftitle={Machine learning with many-body tensor networks | Niko Savola},
}
%   header
\lhead{\textsf{Special exercise}}
\chead{\textsf{PHYS-E0421 - Solid-State Physics}}
\rhead{\textsf{Niko Savola \textbf{653732}}}
%   footer
\lfoot{}
\cfoot{}
\rfoot{\thepage}


\usepackage[braket, qm]{qcircuit}  % Qiskit output

% \setminted{fontsize=\footnotesize, baselinestretch=1}

\begin{document}

\begin{titlepage}
    {\sffamily
    \noindent
    \fontsize{12}{14}\selectfont
    Aalto University \newline
    School of Science \newline
    Department of Applied Physics

    \vspace{40mm}

    \noindent
    \fontsize{14}{16}\selectfont
    \emph{Niko Savola}

    \vspace{10mm}

    \noindent
    \fontsize{18}{22}\selectfont
    \textbf{Machine learning with many-body tensor networks}\\

    \fontsize{12}{14}\selectfont
    \noindent
    Submitted for approval: \today

    \vspace{70mm}

    \noindent
    Special exercise \\[4mm]
    PHYS-E0421 \textendash{} Solid-State Physics \\[4mm]
    } % end of \sffamily

\end{titlepage}
\newpage


\section{Introduction}

Tensor networks have been originally developed for efficiently storing and manipulating high-dimensional quantum many-body states~\cite{PhysRevLett.69.2863}. These variational families of wavefunctions emerge from low-entanglement representations of quantum states. Contemporary applications however include a wide range of fields, such as, machine learning~\cite{Roberts2019}, statistical mechanics~\cite{Levin_2007}, quantum chemistry~\cite{White1999}, and cosmology~\cite{Bao_2017}.
This exercise aims to present the theory behind tensor networks and their use in machine learning. Then we present a numerical example for training a tensor network representing a quantum circuit, effectively pre-training a quantum computer for solving a problem.

% can be used in 
% https://youtu.be/q8UTwdjS95k


\section{Theory}

\subsection{Tensor networks}

Tensor networks (TN) are a powerful tool for splitting a high-dimensional function into constituent parts of smaller tensors. 
Here a tensor refers to a multidimensional array, rank zero corresponding to a scalar, rank one to a vector, rank two to a matrix, and further ranks being referred to as rank $n$ tensors.
It is natural to use Tensor network notation (TNN), which can be considered a graphical generalisation of Einstein summation~\cite{Bridgeman_2017}. This notation maps tensors to  nodes with $n$ lines corresponding to the rank $n$ of the tensor. The lines represent indices for the multidimensional array as demonstrated in \cref{sfig:tensor_basics}.

The elegance of TNN arises from contracting tensors. Multiplication is done by linking the lines of the tensors. For example, matrix-vector multiplication consists of linking a node with one line to a node with two lines. The ensuing tensor will have one free line, resulting in a vector as expected.
This logic is further demonstrated for a few examples in \cref{sfig:tensor_samples}.
Notice how multiplication of high-rank tensors is rendered graphically trivial. This is indeed useful for representing complex architectures of TN\@.



\begin{figure}[htb]
    \centering
    \subfloat[]{{\includegraphics[width=0.43\textwidth]{figures/tensor_diagrams.png}\label{sfig:tensor_basics}}} \qquad
    \subfloat[]{{\includegraphics[width=0.45\textwidth]{figures/sample_contractions.png}\label{sfig:tensor_samples}}}
    
    \caption{\protect\subref{sfig:tensor_basics} Examples of low-rank tensors and their TNN representations. \protect\subref{sfig:tensor_samples} Examples of tensor contractions for matrices. From top to bottom: matrix-vector product, matrix multiplication, and their trace. Note how the last example elegantly results in a scalar. Schematics from Ref.~\cite{Stoudenmire2021}}
    \label{fig:tensor_diagrams}
\end{figure}


\subsubsection{Machine learning}

As popularity of deep learning has risen rapidly~\cite{DL_review}, interest in using TN as replacements or in conjunction with neural networks (NN) has been explored.
Although TN are still an active research topic, some prominent TN architectures for machine learning have emerged, such as, Matrix Product States (MPS) and Tensor Renormalization Group (TRG).

TODO MPS


TODO TRG https://tensornetwork.org/trg/


One branch of research involves using a TN directly as machine learning model architecture. Another uses TNs to compress layers in neural network architectures or for other auxiliary tasks.

a paradigm called differentiable programming.



In fact, it has been shown that there exists a mapping from generative neural networks referred to as Restricted Boltzmann machines to TNs~\cite{PhysRevB.97.085104}, highlighting the link to deep learning.


\subsection{Modelling many-body physics}

todo work as Ans√§tze


% \section{Numerical implementation}


% https://quimb.readthedocs.io/en/latest/examples/ex_tn_train_circuit.html


% \cite{Roberts2019}



\section{Example: Training a quantum circuit}

In this numerical example, we aim to train a quantum circuit for simulating short time evolution of the quantum transverse field Ising model~\cite{Cervera-Lierta2018}. This is done with unitary TN, which can easily be converted to quantum gate operations. The unitarity is ensured with local unitary tensors $v$ obeying $v^\dagger v = vv^\dagger = I$ for identity tensor of the corresponding rank~\cite{Haghshenas2021}.

\subsection{Implementation}

We use an ansatz circuit consisting of quantum U gates and Controlled-Z gates (CZ). The U-gates represent all possible single-qubit operations and are of the form
\begin{align}
    U(\theta, \phi, \lambda) =
            \begin{pmatrix}
                \cos\left(\frac{\theta}{2}\right)          & -e^{i\lambda}\sin\left(\frac{\theta}{2}\right) \\
                e^{i\phi}\sin\left(\frac{\theta}{2}\right) & e^{i(\phi+\lambda)}\cos\left(\frac{\theta}{2}\right)
            \end{pmatrix}
            .
\end{align}
The CZ gates are two-qubit gates flipping the phase if one of the qubits is in the $|1\rangle$ state. It is a symmetric operation and is represented as
\begin{align}
    CZ(q_1, q_0)=
        |0\rangle\langle 0| \otimes I + |1\rangle\langle 1| \otimes \sigma_\text{z} =
        \begin{pmatrix}
            1 & 0 & 0 & 0 \\
            0 & 1 & 0 & 0 \\
            0 & 0 & 1 & 0 \\
            0 & 0 & 0 & -1
        \end{pmatrix}
        ,
\end{align}
where $\sigma_\text{z}$ is a Pauli-Z gate.
Our ansatz is implemented numerically in Python using the \emph{quimb} library~\cite{Gray2018} as
\setminted[python]{fontsize=\scriptsize}
\begin{minted}{python}
import quimb as qu
import quimb.tensor as qtn

n = 5
depth = 4

circ = qtn.Circuit(n)

for d in range(depth):
    for i in range(circ.N):
        params = qu.randn(3, dist='uniform') # initialize with random parameters
        circ.apply_gate('U3', *params, i, gate_round=d, parametrize=True)

    for i in (reversed(regs) if (d % 2 == 0) else regs):
        circ.apply_gate('CZ', i, i + 1, gate_round=d)

# final single qubit layer
for i in range(circ.N):
    params = qu.randn(3, dist='uniform') # initialize with random parameters
    circ.apply_gate('U3', *params, i, gate_round=r, parametrize=True)
\end{minted}
The resulting ansatz consists of exclusively tensors. Thus, a graph complying with TNN presented in \cref{fig:tensor_diagrams} can be generated and are shown in \cref{fig:TNN_circuit} with colouring displaying the quantum gates and qubit indices in \protect\subref{sfig:TNN} and \protect\subref{sfig:TNN_qubits}, respectively.
\begin{figure}[ht]
    \centering
    \subfloat[]{{\includegraphics[width=0.47\textwidth]{figures/tensor_network_graph.pdf}\label{sfig:TNN}}} \quad
    \subfloat[]{{\includegraphics[width=0.47\textwidth]{figures/tensor_network_qubits.pdf}\label{sfig:TNN_qubits}}}
    
    \caption{TNN graph of our ansatz quantum circuit with colouring for \protect\subref{sfig:TNN} quantum gates \protect\subref{sfig:TNN_qubits} qubit indices. The open ends of the TN, labelled with \texttt{k} and \texttt{b}, are consequently linked to a target problem.}
    \label{fig:TNN_circuit}
\end{figure}
The free lines of the TN are consequently connected to a unitary matrix representing the short time evolution of the transverse field Ising model $U(t) = \e^{-itH}$.
This is effectively the target unitary matrix we are trying to replicate with the ansatz.
We implement this in quimb for $t=2$ as follows:
\begin{minted}{python}
H = qu.ham_ising(n, cyclic=False)
U_dense = qu.expm(-1j * (t := 2) * H)
U = qtn.Tensor(
    data=U_dense.reshape([2] * (2 * n)),
    inds=[f'k{i}' for i in range(n)] + [f'b{i}' for i in range(n)],
    tags={'U_TARGET'}
)
\end{minted}

Our loss function to minimise is $L(H_\text{ansatz}) = 1 - \left|\left(H_\text{ansatz}\right)_\gamma U^\gamma \right|$, where $\gamma$ denotes indices for tensor contraction with \emph{tensor index notation}~\cite{Haghshenas2021}.
We accelerate the training with JAX~\cite{jax2018github}, a framework for automatic differentiation and a just-in-time compiler for Graphics processing units (GPU). Limited-memory BFGS (L-BFGS) is chosen as the optimization algorithm~\cite{zhu1997algorithm}. Additionally, basin-hopping is used to avoid local minima by adding random perturbations found minima.



\subsection{Results}

The training was performed using an NVIDIA RTX 3070 GPU for the ansatz circuit with five qubits and four layers of $U$-gate rotations per qubit as shown in \cref{sfig:TNN}.

As the system size was not large, it was possible to solve the exact time evolution of the state $U(t)$ and compare it to the evolution provided by the trained ansatz. This was done for a randomly-initialised state $|\psi\rangle$ by estimating the \emph{fidelity} of the ansatz. As our states are pure, we can define the fidelity to be $\langle U^* \psi | H_\text{ansatz} \psi \rangle$. For this value, we attained $98.94 \%$ from several random trials. Thus, the ansatz time evolution appears competent.

Ultimately, the trained TN was converted back to unitary matrices for quantum circuits. This final circuit is depicted graphically in \cref{fig:qasm_circuit}. As quantum computers with five or more qubits have already been realised~\cite{VTT-IQM}, this circuit could be run on an actual quantum computer. Although it would typically first require transpilation to a native gateset from the general $U$-gates in the noisy intermediate-scale quantum era (NISQ)~\cite{Wilson2020,Li2020}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.87\textwidth]{figures/ansatz_circuit.pdf}
    \caption{Result of the trained quantum ansatz circuit generated with Qiskit~\cite{Qiskit}. The purple blocks represent U-gates with the given $\theta, \phi,$ and $\lambda$ parameters while the blue lines with dots are CZ-gates.}
    \label{fig:qasm_circuit}
\end{figure}



\section{Summary}

Todo
but the motivation is that the architecture generalises to unknown problem





%-------------------
%   Bibliography
%-------------------

\newpage
\pagestyle{plain}
%\setlength\bibitemsep{1.3\itemsep}
\setlength\bibitemsep{0.1\baselineskip}
\addcontentsline{toc}{section}{Viitteet}
\renewcommand*{\bibfont}{\footnotesize}
\printbibliography{}


\end{document}
