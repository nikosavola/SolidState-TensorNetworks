\input{preamble.tex}
\addbibresource{references.bib}
\hypersetup{
    pdftitle={Machine learning with many-body tensor networks | Niko Savola},
}
%   header
\lhead{\textsf{Special exercise}}
\chead{\textsf{PHYS-E0421 - Solid-State Physics}}
\rhead{\textsf{Niko Savola \textbf{653732}}}
%   footer
\lfoot{}
\cfoot{}
\rfoot{\thepage}


\usepackage[braket, qm]{qcircuit}  % Qiskit output

% \setminted{fontsize=\footnotesize, baselinestretch=1}

\begin{document}

\begin{titlepage}
    {\sffamily
    \noindent
    \fontsize{12}{14}\selectfont
    Aalto University \newline
    School of Science \newline
    Department of Applied Physics

    \vspace{40mm}

    \noindent
    \fontsize{14}{16}\selectfont
    \emph{Niko Savola}

    \vspace{10mm}

    \noindent
    \fontsize{18}{22}\selectfont
    \textbf{Machine learning with many-body tensor networks}\\

    \fontsize{12}{14}\selectfont
    \noindent
    Submitted for approval: \today

    \vspace{70mm}

    \noindent
    Special exercise \\[4mm]
    PHYS-E0421 \textendash{} Solid-State Physics \\[4mm]
    } % end of \sffamily

\end{titlepage}
\newpage


\section{Introduction}

Tensor networks have been originally developed for efficiently storing and manipulating high-dimensional quantum many-body states~\cite{PhysRevLett.69.2863}. These variational families of wavefunctions emerge from low-entanglement representations of quantum states. Contemporary applications however include a wide range of fields, such as, machine learning~\cite{Roberts2019}, statistical mechanics~\cite{Levin_2007}, quantum chemistry~\cite{White1999}, and cosmology~\cite{Bao_2017}.
This exercise aims to present the theory behind tensor networks and their use in machine learning. Then we present a numerical example for training a tensor network representing a quantum circuit, effectively pre-training a quantum computer for solving a problem.

% can be used in 


% TODO review paragraph.
% https://youtu.be/q8UTwdjS95k


\section{Theory}

\subsection{Tensor networks}

Tensor networks (TN) are a powerful tool for splitting a high-dimensional function into constituent parts of smaller tensors. 
Here a tensor refers to a multidimensional array, rank zero corresponding to a scalar, rank one to a vector, rank two to a matrix, and further ranks being referred to as rank $n$ tensors.
It is natural to use Tensor network notation (TNN), which can be considered a graphical generalisation of Einstein summation~\cite{Bridgeman_2017}. This notation maps tensors to  nodes with $n$ lines corresponding to the rank $n$ of the tensor. The lines represent indices for the multidimensional array as demonstrated in \cref{sfig:tensor_basics}.

The elegance of TNN arises from contracting tensors. Multiplication is done by linking the lines of the tensors. For example, matrix-vector multiplication consists of linking a node with one line to a node with two lines. The ensuing tensor will have one free line, resulting in a vector as expected.
This logic is further demonstrated for a few examples in \cref{sfig:tensor_samples}.
Notice how multiplication of high-rank tensors is rendered graphically trivial. This is indeed useful for representing complex architectures of TN.



\begin{figure}[htb]
    \centering
    \subfloat[]{{\includegraphics[width=0.43\textwidth]{figures/tensor_diagrams.png}\label{sfig:tensor_basics}}} \qquad
    \subfloat[]{{\includegraphics[width=0.45\textwidth]{figures/sample_contractions.png}\label{sfig:tensor_samples}}}
    
    \caption{\protect\subref{sfig:tensor_basics} Examples of low-rank tensors and their TNN representations. \protect\subref{sfig:tensor_samples} Examples of tensor contractions for matrices. From top to bottom: matrix-vector product, matrix multiplication, and their trace. Note how the last example elegantly results in a scalar. Schematics from Ref.~\cite{Stoudenmire2021}}
    \label{fig:tensor_diagrams}
\end{figure}

% TODO notation from https://arxiv.org/pdf/1905.01330.pdf


\subsubsection{Machine learning}

As popularity of deep learning has risen rapidly~\cite{DL_review}, interest in using TN as replacements or in conjunction with neural networks (NN) has been explored.
Although TN are still an active research topic, some prominent TN architectures for machine learning have emerged, such as, Matrix Product States (MPS) and Tensor Renormalization Group (TRG).

TODO MPS


TODO TRG https://tensornetwork.org/trg/


One branch of research involves using a TN directly as machine learning model architecture. Another uses TNs to compress layers in neural network architectures or for other auxiliary tasks.

a paradigm called differentiable programming.



In fact, it has been shown that there exists a mapping from generative neural networks referred to as Restricted Boltzmann machines to TNs~\cite{PhysRevB.97.085104}, highlighting the link to deep learning.


\subsection{Modelling many-body physics}

todo work as Ans√§tze


% \section{Numerical implementation}


% https://quimb.readthedocs.io/en/latest/examples/ex_tn_train_circuit.html


% \cite{Roberts2019}



\section{Example: Training a quantum circuit}

TODO EXPLAIN WHAT WE ARE DOING

\subsection{Implementation}

We use an ansatz circuit consisting of quantum U gates and Controlled-Z gates (CZ). The U-gates represent all possible single-qubit operations and are of the form
\begin{align}
    U(\theta, \phi, \lambda) =
            \begin{pmatrix}
                \cos\left(\frac{\theta}{2}\right)          & -e^{i\lambda}\sin\left(\frac{\theta}{2}\right) \\
                e^{i\phi}\sin\left(\frac{\theta}{2}\right) & e^{i(\phi+\lambda)}\cos\left(\frac{\theta}{2}\right)
            \end{pmatrix}
            .
\end{align}
The CZ gates are two-qubit gates flipping the phase if one of the qubits is in the $|1\rangle$ state. It is a symmetric operation and is represented as
\begin{align}
    CZ(q_1, q_0)=
        |0\rangle\langle 0| \otimes I + |1\rangle\langle 1| \otimes \sigma_\text{z} =
        \begin{pmatrix}
            1 & 0 & 0 & 0 \\
            0 & 1 & 0 & 0 \\
            0 & 0 & 1 & 0 \\
            0 & 0 & 0 & -1
        \end{pmatrix}
        ,
\end{align}
where $\sigma_\text{z}$ is a Pauli-Z gate.
Our ansatz is implemented numerically in Python using the \emph{quimb} library~\cite{Gray2018} as
\setminted[python]{fontsize=\scriptsize}
\begin{minted}{python}
import quimb as qu
import quimb.tensor as qtn

n = 5
depth = 4

circ = qtn.Circuit(n)

for d in range(depth):
    for i in range(circ.N):
        params = qu.randn(3, dist='uniform') # initialize with random parameters
        circ.apply_gate('U3', *params, i, gate_round=d, parametrize=True)

    for i in (reversed(regs) if (d % 2 == 0) else regs):
        circ.apply_gate('CZ', i, i + 1, gate_round=d)

# final single qubit layer
for i in range(circ.N):
    params = qu.randn(3, dist='uniform') # initialize with random parameters
    circ.apply_gate('U3', *params, i, gate_round=r, parametrize=True)
\end{minted}
The resulting ansatz consists of exclusively tensors. Thus, a graph complying with TNN presented in \cref{fig:tensor_diagrams} can be generated. Similar graphs are shown in \cref{fig:TNN_circuit} with colouring displaying the quantum gates and qubit indices in \protect\subref{sfig:TNN} and \protect\subref{sfig:TNN_qubits}, respectively.

\begin{figure}[ht]
    \centering
    \subfloat[]{{\includegraphics[width=0.47\textwidth]{figures/tensor_network_graph.pdf}\label{sfig:TNN}}} \quad
    \subfloat[]{{\includegraphics[width=0.47\textwidth]{figures/tensor_network_qubits.pdf}\label{sfig:TNN_qubits}}}
    
    \caption{TNN graph of our ansatz quantum circuit with colouring for \protect\subref{sfig:TNN} quantum gates \protect\subref{sfig:TNN_qubits} qubit indices. The open ends of the TN, labelled with \texttt{k} and \texttt{b}, are consequently linked to a target problem.}
    \label{fig:TNN_circuit}
\end{figure}

The TN is then connected to 
TODO qu.ham\_ising




\subsection{Results}


\begin{figure}[htb]
    \centering

    \includegraphics[width=0.97\textwidth]{figures/ansatz_circuit.pdf}

    \caption{Result of the trained quantum ansatz circuit generated with Qiskit~\cite{Qiskit}. The purple blocks represent U-gates, while the blue lines with dots are Controlled-Z gates. The U-gates are the most general form of single-qubit gates and thus on a physical device might be implemented as a combination of different gates depending on the given rotations in the noisy intermediate-scale quantum era (NISQ)~\cite{Li2020}.}
    \label{fig:qasm_circuit}
\end{figure}



todo suprajohto qubit toteutus
https://arxiv.org/pdf/2103.12305.pdf




but the motivation is that the architecture generalises to unknown problem

\section{Summary}

Todo





TODO https://github.com/google/TensorNetwork

https://arxiv.org/pdf/1905.01331.pdf

%-------------------
%   Bibliography
%-------------------

\newpage
\pagestyle{plain}
%\setlength\bibitemsep{1.3\itemsep}
\setlength\bibitemsep{0.1\baselineskip}
\addcontentsline{toc}{section}{Viitteet}
\renewcommand*{\bibfont}{\footnotesize}
\printbibliography{}


\end{document}
